# ğŸ“˜ Creating and Managing LakeFlow Jobs using UI

This Databricks lab demonstrates how to use the **LakeFlow Jobs UI** to configure and deploy a **multi-step machine learning workflow** with manual triggers and email notifications.

---

## âœ… Objectives

You will complete the following tasks:

- ğŸ”§ Create a LakeFlow Job with multiple tasks using the UI
- ğŸ“§ Enable email notifications for job status updates
- ğŸ–±ï¸ Manually trigger the deployment workflow
- ğŸ“Š Monitor the run status to verify successful execution

---

## ğŸ’» Prerequisites

Before running the notebook:

- Attach to a **Classic Compute** cluster (not Serverless)
- If needed, restart the cluster and select it manually

**Steps:**

1. Click compute selector (top-right)
2. Choose a classic compute cluster
3. If not listed: _More â†’ Select cluster manually_

---

## ğŸ› ï¸ Workflow Summary

This notebook is not code-heavy. It focuses on **UI-based job creation**.

| Component          | Description                                 |
|--------------------|---------------------------------------------|
| LakeFlow Job       | Created via Databricks UI                   |
| Tasks              | Include notebook runs and model deployment |
| Email Notifications| Enabled for success/failure alerts         |
| Manual Trigger     | Used to run the workflow on demand         |
| Monitoring         | Checked via UI and job run history         |

---

## ğŸ§ª Skills Practiced

- Using the Databricks UI to create multi-step jobs
- Understanding task dependencies and flows
- Configuring email notifications
- Monitoring and debugging job runs
