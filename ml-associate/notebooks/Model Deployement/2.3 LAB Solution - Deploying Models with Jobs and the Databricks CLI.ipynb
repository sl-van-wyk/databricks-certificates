{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "315ddd86-0915-40f5-b2f0-67e90640c339",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be32f3cc-ee25-4f4d-85c6-8ddaf5c566ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Lab- Deploying Models with Jobs and the Databricks CLI\n",
    "\n",
    "In this Lab, you will update the alias of a previously created model to **\"Champion\"**, signifying its readiness for deployment. Utilizing the **Databricks CLI**, you will construct and initiate a **Lakeflow Jobs**. This job will deploy the latest model version marked **\"Champion\"** if it meets the production-ready criteria, followed by executing a **batch inference** process.\n",
    "\n",
    "**Lab Outline:**\n",
    "\n",
    "_In this lab, you will complete the following tasks:_\n",
    "- **Task 1:** Identify and update a model's alias to **\"Champion\"**.\n",
    "- **Task 2:** Configure and use the Databricks CLI to manage jobs.\n",
    "- **Task 3:** Create and run a job for model deployment and Batch Inferencing.\n",
    "- **Task 4:** Monitor and explore the executing job.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2baedb41-200e-4dab-8802-b9676ad047ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "Follow these steps to select the classic compute cluster:\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "1. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "   - In the drop-down, select **More**.\n",
    "   - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.\n",
    "  \n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "1. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "1. Wait a few minutes for the cluster to start.\n",
    "1. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd73e2f1-a36b-485c-bc72-ee085e743c42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need to use one of the following Databricks runtime(s): **16.3.x-cpu-ml-scala2.12**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f161d575-5bc6-4a00-b8d6-3edcc88803b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Classroom Setup\n",
    "\n",
    "Before starting the demo, run the provided classroom setup script. This script will define configuration variables necessary for the demo. Execute the following cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b7724a7-37a4-4f19-a470-444f07c6f4a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting databricks-sdk==0.36.0\n  Using cached databricks_sdk-0.36.0-py3-none-any.whl.metadata (38 kB)\nRequirement already satisfied: requests<3,>=2.28.1 in /databricks/python3/lib/python3.12/site-packages (from databricks-sdk==0.36.0) (2.32.2)\nRequirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-sdk==0.36.0) (2.21.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk==0.36.0) (5.3.3)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk==0.36.0) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk==0.36.0) (4.9)\nRequirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from google-auth~=2.0->databricks-sdk==0.36.0) (1.16.0)\nRequirement already satisfied: urllib3<2.0 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk==0.36.0) (1.26.16)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.28.1->databricks-sdk==0.36.0) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.28.1->databricks-sdk==0.36.0) (3.7)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.28.1->databricks-sdk==0.36.0) (2024.6.2)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk==0.36.0) (0.4.8)\nUsing cached databricks_sdk-0.36.0-py3-none-any.whl (569 kB)\nInstalling collected packages: databricks-sdk\n  Attempting uninstall: databricks-sdk\n    Found existing installation: databricks-sdk 0.30.0\n    Not uninstalling databricks-sdk at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-b3292ff6-6449-49c2-8164-d51a69240874\n    Can't uninstall 'databricks-sdk'. No files were found to uninstall.\nSuccessfully installed databricks-sdk-0.36.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%run ../Includes/Classroom-Setup-02Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "795742bf-cc50-45e7-b611-0175252f08ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "647f91f8-b368-4fe2-af76-65877c35a6f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username:          labuser10859621_1751977661@vocareum.com\nCatalog Name:      dbacademy\nSchema Name:       labuser10859621_1751977661\nWorking Directory: /Volumes/dbacademy/ops/labuser10859621_1751977661@vocareum_com\nDataset Location:  NestedNamespace (banking='/Volumes/dbacademy_banking/v01', cdc_diabetes='/Volumes/dbacademy_cdc_diabetes/v01', monitoring='/Volumes/dbacademy_monitoring/v01', telco='/Volumes/dbacademy_telco/v01')\n"
     ]
    }
   ],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a4765bc-ec03-4bea-a2b5-3892c9e2e588",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Authentication\n",
    "\n",
    "Usually, you would have to set up authentication for the CLI. But in this training environment, that's already taken care of if you ran through the accompanying \n",
    "**'Generate Tokens'** notebook. \n",
    "If you did, credentials will already be loaded into the **`DATABRICKS_HOST`** and **`DATABRICKS_TOKEN`** environment variables. \n",
    "\n",
    "#####*If you did not, run through it now then restart this notebook.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0c05632-be57-4263-9d12-0b7d418763ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ed82dc393ed40a48f62cda703e356a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='https://dbc-ef9ac081-5ec6.cloud.databricks.com', continuous_update=False, deâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DA.get_credentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "073d1a24-54f6-490e-aa4c-3810a9c5595b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Install CLI\n",
    "\n",
    "Install the Databricks CLI using the following cell. Note that this procedure removes any existing version that may already be installed, and installs the newest version of the [Databricks CLI](https://docs.databricks.com/en/dev-tools/cli/index.html). A legacy version exists that is distributed through **`pip`**, however we recommend following the procedure here to install the newer one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "104c539a-8642-44e7-8bdd-4f7a42190922",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed Databricks CLI v0.211.0 at /usr/local/bin/databricks.\n"
     ]
    }
   ],
   "source": [
    "%sh rm -f $(which databricks); curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/v0.211.0/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f644176e-024b-40ae-a3a2-e9fc707951f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Notebook Path Setup Continued\n",
    "\n",
    "This code cell performs the following setup tasks:\n",
    "- Retrieves the current Databricks **cluster ID** and displays it.\n",
    "- Identifies the path of the currently running notebook.\n",
    "- Constructs **paths to related notebooks** for checking model status, deploying the model, performing model inference, and handling cases where the model is not ready for production. These paths are printed to confirm their accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7816cad8-65ac-466b-be99-535f5bc346b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0708-122829-du6hmtbj\n/Users/labuser10859621_1751977661@vocareum.com/machine-learning-operations-2.2.4/M02 - Architecting MLOps Solutions/2.3 Lab Pipeline - Status Check and Deployment/2.3a LAB - Checking Model Status\n/Users/labuser10859621_1751977661@vocareum.com/machine-learning-operations-2.2.4/M02 - Architecting MLOps Solutions/2.3 Lab Pipeline - Status Check and Deployment/2.3b LAB - Model Deployment\n/Users/labuser10859621_1751977661@vocareum.com/machine-learning-operations-2.2.4/M02 - Architecting MLOps Solutions/2.3 Lab Pipeline - Status Check and Deployment/2.3c LAB - Batch Inferencing\n/Users/labuser10859621_1751977661@vocareum.com/machine-learning-operations-2.2.4/M02 - Architecting MLOps Solutions/2.3 Lab Pipeline - Status Check and Deployment/2.3d LAB - Not Ready For Production\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the current cluster ID\n",
    "cluster_id = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterId\")\n",
    "print(cluster_id)\n",
    "\n",
    "# Get the current notebook path\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "\n",
    "# Base path for related notebooks\n",
    "base_path = notebook_path.rsplit('/', 1)[0] + \"/2.3 Lab Pipeline - Status Check and Deployment\"\n",
    "\n",
    "# Paths for specific process notebooks\n",
    "check_status_notebook_path = f\"{base_path}/2.3a LAB - Checking Model Status\"\n",
    "print(check_status_notebook_path)\n",
    "\n",
    "deploy_notebook_path = f\"{base_path}/2.3b LAB - Model Deployment\"\n",
    "print(deploy_notebook_path)\n",
    "\n",
    "inference_notebook_path = f\"{base_path}/2.3c LAB - Batch Inferencing\"\n",
    "print(inference_notebook_path)\n",
    "\n",
    "notready_notebook_path = f\"{base_path}/2.3d LAB - Not Ready For Production\"\n",
    "print(notready_notebook_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3be558a9-9003-444e-841b-3b3f4fc698ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Task 1: Identify Model as Ready for Deployment\n",
    "\n",
    "1. Navigate to **Models** in the left sidebar.\n",
    "2. Apply the filter for **models Owned by Me**.\n",
    "3. Locate and select the model named **churn-prediction**.\n",
    "   - **Note:** Complete the Job in **Notebook: 1.2 LAB - Creating and Managing Lakeflow Jobs using UI** to create the model. If the model isn't listed, verify that your first Job Run is complete.\n",
    "4. Review the information provided when your model is registered in the Unity Catalog's Model Registry.\n",
    "5. Click the pencil icon next to the alias **\"baseline\"** and change it to **\"champion\"**. Save the alias.\n",
    "\n",
    "**_This step identifies your model as ready for deployment._**\n",
    "\n",
    "<!-- <img src=\"https://s3.us-west-2.amazonaws.com/files.training.databricks.com/images/Model_Deploy_Alias_Change.png\" alt=\"Alias Change\" width=\"700\"/> -->\n",
    "\n",
    "![Baseline](../Includes/images/Baseline.png)\n",
    "\n",
    "![Edit_alias](../Includes/images/Edit_alias.png)\n",
    "\n",
    "![Champion](../Includes/images/Champion.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bb2ee7f-220d-41b9-99b4-aee432d49f14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 2: Configuration of a Lakeflow Jobs\n",
    "\n",
    "This code cell constructs a JSON configuration string for a Databricks Lakeflow Jobs. The configuration specifies a series of tasks designed to handle the deployment and inference stages for a model called **\"churn-prediction\"**. Here are the key components and their functions:\n",
    "\n",
    "- **General Settings**:\n",
    "  - The job runs sequentially with a **maximum of one concurrent run** and is named using the current user's username with the suffix `-deploy-workflow-job`.\n",
    "  - It includes **email notifications** on failure, configured to alert the user.\n",
    "\n",
    "- **Tasks Defined**:\n",
    "  - **Check Status**: Verifies if the model is ready for production by checking its status.\n",
    "  - **Production Ready**: Proceeds if the model status is 'ready for production'.\n",
    "  - **Deploy**: Deploys the model if the previous task confirms readiness.\n",
    "  - **Batch Inference**: Executes a batch inference process following successful deployment.\n",
    "  - **Not Ready**: Handles scenarios where the model is not ready for production.\n",
    "\n",
    "- **Conditional Execution**:\n",
    "  - Each task, except for the initial status check, includes conditions that depend on the success of the preceding tasks.\n",
    "  - If conditions are met, the next task in the Job is triggered, otherwise, alternative actions are specified.\n",
    "\n",
    "- **File Operations**:\n",
    "  - The constructed JSON string is written to a file named `workflow-job-lab.json` in write mode, ensuring that the entire Job configuration is saved externally for deployment purposes.\n",
    "\n",
    "This setup is essential for automating model deployment workflows in a controlled and predictable manner, allowing for efficient scaling and maintenance of machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02d23718-8f21-4073-9a3c-0e4115b10d09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "workflow_config = f\"\"\"\n",
    "{{\n",
    "  \"email_notifications\": {{\n",
    "    \"on_failure\": [\n",
    "      \"{DA.username}\"\n",
    "    ]\n",
    "  }},\n",
    "  \"format\": \"MULTI_TASK\",\n",
    "  \"max_concurrent_runs\": 1,\n",
    "  \"name\": \"{DA.username}-deploy-workflow-job\",\n",
    "  \"notification_settings\": {{\n",
    "    \"alert_on_last_attempt\": false,\n",
    "    \"no_alert_for_canceled_runs\": false,\n",
    "    \"no_alert_for_skipped_runs\": false\n",
    "  }},\n",
    "  \"tasks\": [\n",
    "    {{\n",
    "      \"existing_cluster_id\": \"{cluster_id}\",\n",
    "      \"notebook_task\": {{\n",
    "        \"notebook_path\": \"{check_status_notebook_path}\",\n",
    "        \"source\": \"WORKSPACE\"\n",
    "      }},\n",
    "      \"run_if\": \"ALL_SUCCESS\",\n",
    "      \"task_key\": \"check_status\"\n",
    "    }},\n",
    "    {{\n",
    "      \"condition_task\": {{\n",
    "        \"left\": \"{{{{tasks.check_status.values.model_status}}}}\",\n",
    "        \"op\": \"EQUAL_TO\",\n",
    "        \"right\": \"ready_for_production\"\n",
    "      }},\n",
    "      \"depends_on\": [\n",
    "        {{\n",
    "          \"task_key\": \"check_status\"\n",
    "        }}\n",
    "      ],\n",
    "      \"email_notifications\": {{}},\n",
    "      \"notification_settings\": {{\n",
    "        \"alert_on_last_attempt\": false,\n",
    "        \"no_alert_for_canceled_runs\": false,\n",
    "        \"no_alert_for_skipped_runs\": false\n",
    "      }},\n",
    "      \"run_if\": \"ALL_SUCCESS\",\n",
    "      \"task_key\": \"Production_Ready\",\n",
    "      \"timeout_seconds\": 0,\n",
    "      \"webhook_notifications\": {{}}\n",
    "    }},\n",
    "    {{\n",
    "      \"depends_on\": [\n",
    "        {{\n",
    "          \"outcome\": \"true\",\n",
    "          \"task_key\": \"Production_Ready\"\n",
    "        }}\n",
    "      ],\n",
    "      \"email_notifications\": {{}},\n",
    "      \"existing_cluster_id\": \"{cluster_id}\",\n",
    "      \"notebook_task\": {{\n",
    "        \"notebook_path\": \"{deploy_notebook_path}\",\n",
    "        \"source\": \"WORKSPACE\"\n",
    "      }},\n",
    "      \"notification_settings\": {{\n",
    "        \"alert_on_last_attempt\": false,\n",
    "        \"no_alert_for_canceled_runs\": false,\n",
    "        \"no_alert_for_skipped_runs\": false\n",
    "      }},\n",
    "      \"run_if\": \"ALL_SUCCESS\",\n",
    "      \"task_key\": \"Deploy\",\n",
    "      \"timeout_seconds\": 0,\n",
    "      \"webhook_notifications\": {{}}\n",
    "    }},\n",
    "    {{\n",
    "      \"depends_on\": [\n",
    "        {{\n",
    "          \"task_key\": \"Deploy\"\n",
    "        }}\n",
    "      ],\n",
    "      \"email_notifications\": {{}},\n",
    "      \"existing_cluster_id\": \"{cluster_id}\",\n",
    "      \"notebook_task\": {{\n",
    "        \"notebook_path\": \"{inference_notebook_path}\",\n",
    "        \"source\": \"WORKSPACE\"\n",
    "      }},\n",
    "      \"notification_settings\": {{\n",
    "        \"alert_on_last_attempt\": false,\n",
    "        \"no_alert_for_canceled_runs\": false,\n",
    "        \"no_alert_for_skipped_runs\": false\n",
    "      }},\n",
    "      \"run_if\": \"ALL_SUCCESS\",\n",
    "      \"task_key\": \"Batch_Inference\",\n",
    "      \"timeout_seconds\": 0,\n",
    "      \"webhook_notifications\": {{}}\n",
    "    }},\n",
    "    {{\n",
    "      \"depends_on\": [\n",
    "        {{\n",
    "          \"outcome\": \"false\",\n",
    "          \"task_key\": \"Production_Ready\"\n",
    "        }}\n",
    "      ],\n",
    "      \"email_notifications\": {{}},\n",
    "      \"existing_cluster_id\": \"{cluster_id}\",\n",
    "      \"notebook_task\": {{\n",
    "        \"notebook_path\": \"{notready_notebook_path}\",\n",
    "        \"source\": \"WORKSPACE\"\n",
    "      }},\n",
    "      \"notification_settings\": {{\n",
    "        \"alert_on_last_attempt\": false,\n",
    "        \"no_alert_for_canceled_runs\": false,\n",
    "        \"no_alert_for_skipped_runs\": false\n",
    "      }},\n",
    "      \"run_if\": \"ALL_SUCCESS\",\n",
    "      \"task_key\": \"Not_Ready\",\n",
    "      \"timeout_seconds\": 0,\n",
    "      \"webhook_notifications\": {{}}\n",
    "    }}\n",
    "  ],\n",
    "  \"queue\": {{\n",
    "    \"enabled\": true\n",
    "  }},\n",
    "  \"run_as\": {{\n",
    "    \"user_name\": \"{DA.username}\"\n",
    "  }}\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "with open('workflow-job-lab.json', 'w') as file:\n",
    "    file.write(workflow_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "617f4e85-a42d-41a7-8312-85eb2c893a87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 3: Creating and Running a Lakeflow Jobs\n",
    "\n",
    "This section details the process of creating and executing a Lakeflow Jobs using the Databricks CLI:\n",
    "\n",
    "1. **Creating the Job**:\n",
    "   - The job is created by passing the JSON configuration file `workflow-job-lab.json` to the `databricks jobs create` command. This command returns a JSON object containing details of the created job, including the `job_id`.\n",
    "\n",
    "2. **Extracting the Job ID**:\n",
    "   - The `job_id` is extracted from the JSON output using a combination of `grep` and `awk`. The `grep` command isolates the line containing `job_id`, and `awk` is used to select the second field (the actual ID value), which is then stripped of extra characters using `tr`.\n",
    "\n",
    "3. **Running the Job**:\n",
    "   - With the `job_id` extracted, the job is initiated using `databricks jobs run-now`. This command triggers the execution of the workflow defined in the job configuration file.\n",
    "   \n",
    "This process automates the deployment of tasks defined in the Databricks environment, ensuring that the model deployment and associated tasks are handled efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6632aad3-ecd5-43dd-a0aa-9b1dd7f9fbe5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ \"job_id\":816473192443836 }\nExtracted job_id: 816473192443836\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: failed to reach TERMINATED or SKIPPED, got INTERNAL_ERROR: Task check_status failed with message: Workload failed, see run output for details. This caused all downstream tasks to get skipped.\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "## Create the job and capture the output\n",
    "output=$(databricks jobs create --json @workflow-job-lab.json)\n",
    "echo $output\n",
    "## Extract the job_id from the output\n",
    "job_id=$(echo $output | grep -o '\"job_id\":[0-9]*' | awk -F':' '{print $2}')\n",
    "echo \"Extracted job_id: $job_id\"\n",
    "\n",
    "## Run the job using the extracted job_id\n",
    "databricks jobs run-now $job_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ca9bdcb-9f1d-45ab-8279-ae0887afdd6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Task 4:  Monitoring and Exploring the Executing Lakeflow Jobs\n",
    "\n",
    "To effectively manage and gain insights from your executing Lakeflow Jobs in the Databricks environment, follow these steps:\n",
    "\n",
    "1. **Access the Jobs Console**:\n",
    "   - From the Databricks sidebar, navigate to the **Job Runs** section, which lists all configured jobs.\n",
    "\n",
    "2. **Find and View the Job**:\n",
    "   - Use the `job_id` (``) or `{DA.username}-deploy-workflow-job` to locate your job. Click on the job name to access its details page.\n",
    "\n",
    "3. **Explore Tasks**:\n",
    "   - The job's details page displays its current status (e.g., *running*, *success*, *failure*). Click on a **task tab** to view the created Job.  Click on each tasks to see the details.\n",
    "\n",
    "4. **Explore Run Outputs**:\n",
    "   - Go back to the **Run Tab** click on the Runs.  Investigate the output logs, metrics, etc. of tasks for debugging information or to verify successful execution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dbc0dec-185a-400b-bccf-3110bb9c6f30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Conclusion\n",
    "In this lab, you successfully set up and executed a Lakeflow Jobs using the Databricks CLI. You configured the job to check the model status, deploy the model, perform batch inference, and handle scenarios where the model is not ready for production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc2d9cc4-f010-463c-8a6f-5fcb98ab18bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2025 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"blank\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\" target=\"blank\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\" target=\"blank\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\" target=\"blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3269837496522882,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2.3 LAB Solution - Deploying Models with Jobs and the Databricks CLI",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}